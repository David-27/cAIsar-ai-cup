import numpy as np
import pandas as pd
from typing import List, Tuple
import itertools


def concat_ts_partials(partials: List[np.array], center: int, center_offset: int) -> np.array:
    """For predicting a dataset that is longer than one batch frame, results from multiple 
    predictions along the timeline have to be concatenated. For further 
    documentation / explanation refer to  :ref:`Reformat predictions.ipynb`

    Args:
        partials (List[np.array]): The list of predictions along the timeline, 
        the first element being the prediction at the start of the timeline
        center (int): The center of a prediction
        center_offset (int): The offset of the prediction center to the prediction start 

    Returns:
        np.array: concatenated time-series
    """

    concatenated = np.array([])
    for i, p in enumerate(partials):
        rel_start = center_offset if (i != 0) else 0
        rel_end = center_offset + center if (i != len(partials) - 1) \
            else center + 2 * center_offset
        if len(concatenated) == 0:
            concatenated = p[rel_start:rel_end]
        else:
            concatenated = np.concatenate(
                (concatenated, p[rel_start:rel_end]), axis=0)

    return concatenated


def pad_array(arr: np.array, sample_offset: int, length: int, sample_freq: int, fill: int = 0):
    """Adds padding to a pd.DataFrame to fit the batch size. 

    Args:
        df (pd.DataFrame): The pd.DataFrame, to which the padding should be added
        sample_offset (int): The offset of the starting index to the data generation, typcal value is 0
        length (int): The length of one data row of a batch, typical value is 1024
        sample_freq (int): The frequency of sampling along the time-axis. 
        To produce more training material, not the full length is used but a number < length. 
        This results in specific datapoints will be included in multiple data frames
        fill (int, optional): The value that should be used as fill. Defaults to 0.

    Returns:
        pd.Dataframe: A pd.DataFrame which has added rows until it fits perfectly the batch generation 
    """
    index = sample_offset
    while index + length < len(arr):
        index += sample_freq
    if index - len(arr) > 0:
        padding = np.full(shape=index - len(arr),
                          fill_value=fill, dtype=np.int)
        padded = np.concatenate([arr, padding])
        return padded
    return arr


def pad_df(df: pd.DataFrame, sample_offset: int, length: int, sample_freq: int, fill: int = 0) -> pd.DataFrame:
    """
    Adds padding to a pd.DataFrame to fit the batch size. 

    Args:
        df (pd.DataFrame): The pd.DataFrame, to which the padding should be added
        sample_offset (int): The offset of the starting index to the data generation, typcal value is 0
        length (int): The length of one data row of a batch, typical value is 1024
        sample_freq (int): The amount of indexes moved to the right by sample. 
        To produce more training material, not the full length is used but a number < length. 
        This results in specific datapoints will be included in multiple data frames
        fill (int, optional): The value that should be used as fill. Defaults to 0.

    Returns:
        pd.Dataframe: A pd.DataFrame which has added rows until it fits perfectly the batch generation 
    """

    df = df.copy(deep=True)

    # index of the last start index for a batch
    index = sample_offset
    while index + length < df.shape[0]:
        index += sample_freq
    if (index + length) - df.shape[0] > 0:
        padding = pd.DataFrame(fill, index=np.arange(
            df.shape[0], (index + length)), columns=df.columns)
        padded = pd.concat([df, padding])
        return padded
    return df



def convert_batches_to_ts(data: np.array, batch_table: List[List[Tuple[str, int, List[int]]]], center: int, 
            center_offset: int, sample_freq: int, ignore: List[str] = []) -> List[np.array]:
    """
    Converts a set of batches of data/predictions to a list of time-series
    NOTE: previously, this logic required that no part of a time series comes before a later
        this has been adapted; shuffling is now allowed

    Args:
        data (np.array): the data to be converted of the shape (batches, batch_size, ...)
        batch_table (List[List[Tuple[str, int, List[int]]]]): a tavble containing the batch information,
            automatically generated by BatchGenerator
        center (int): The center of a prediction
        center_offset (int): The offset of the prediction center to the prediction start 

    Returns:
        List[np.array]: a list containing the different time-series
    """

    time_series = {}

    for batch_nr in range(len(batch_table)):
        # iterate through the batch_table to find the different time_series by name
        batch_info: List[Tuple[str, int, List[int]]] = batch_table[batch_nr]

        # iterate through the different parts that may constitute one batch
        # e. g.     [('C:\\path\\data\\1-1.csv', 2, [2048, 2560]),
        #            ('C:\\path\\data\\1-2.csv', 2, [0, 512])]

        offset: int = 0
        for piece in batch_info:
            file, _, indices = piece

            if file in ignore:
                continue

            # create an entry for every time_series
            if file not in time_series.keys():
                time_series[file] = {}

            # create an entry for every part of the time_series corresponding to
            # the position within a batch and the position in the time_series
            # e. g. {'C:\\path\\data\\1-1.csv':
            #       {0: [(0, 512), (1, 0), (2, 1024), (3, 1536)],
            #          1: [(0, 2560), (1, 2048)]}
            time_series[file][batch_nr] = [
                (i, indices[i - offset]) for i in range(offset, len(indices) + offset)]
            offset += len(indices)

    collected_ts = []

    # iterate over all time series
    for key in time_series.keys():
        if key in ignore:
            continue
            
        parts_dict = time_series[key]
        max_pos = None
        for batch_nr in parts_dict.keys():
            indices = parts_dict[batch_nr]
            positions = list(map(lambda x: x[1], indices))
            if max_pos == None:
                max_pos = max(positions)
            if max(positions) > max_pos:
                max_pos = max(positions)    
        
        total = int(max_pos / sample_freq) + 1
        parts = [None] * total
        
        # iterate over the batches containing parts of the time series
        for batch_nr in parts_dict.keys():
            batch = data[batch_nr]
            indices = parts_dict[batch_nr]
            # append the part at the respective index
            for index, pos_in_ts in indices:
                parts[int(pos_in_ts / sample_freq)] = batch[index]
        
        
        # adapted itertools-solution from
        # https://stackoverflow.com/questions/60424282/split-a-list-in-python-with-an-element-as-the-delimiter#60424401
        array_or_none = lambda x: not(type(x) == np.ndarray)
        subparts = [list(value) for key, value in itertools.groupby(parts, array_or_none) if not key]
        
        # concatenate the time-series parts to one array according to center and center_offset
        for subpart in subparts:
            concat_ts = concat_ts_partials(
                partials=subpart, center=center, center_offset=center_offset)
            collected_ts.append(concat_ts)

    return collected_ts

def stack_batches(data: np.array) -> np.array:
    collector = []
    for batch in data:
        collector.append(batch)
    stacked = np.vstack(collector)
    return stacked

"""
This is the buggy version of unstack_batches. !!! DANGER !!!
def unstack_batches(data: np.array, batch_size: int) -> np.array:
    collector = []
    for index in range(data.shape[0] // batch_size):
        collector.append(data[index:index+batch_size])
    unstacked = np.array(collector)
    return unstacked
"""

# this one should work
def unstack_batches(data: np.array, batch_size: int) -> np.array:
    collector = []
    for index in range(0,len(data),batch_size):
        collector.append(data[index:index+batch_size])
    unstacked = np.array(collector)
    return unstacked
